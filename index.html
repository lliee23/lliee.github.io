<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Luking Li</title>
    <meta content="Luking Li, https://github/liai" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Times New Roman;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Times New Roman;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Times New Roman;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Times New Roman;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Times New Roman;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 110px;"
             src="./lujunli.jpg">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Lujun LI (李路军)</span><br>
            <span> Hong Kong University of Science and Technology (HKUST)</span><br>
<span><strong>Address</strong>: Clear Water Bay Peninsula, New Territories, Hong Kong</span><br>
            <span><strong>Email</strong>: lilujunai@gmail.com, lliee@ust.hk </span> <br>
                     <span> <a href="https://openreview.net/profile?id=~Lujun_Li1">Openreview</a></span> <br>
            <span> <a href="https://scholar.google.com/citations?hl=en&user=aPl3DjIAAAAJ&view_op=list_works&gmla=AJsN-F6PzWqNjlzy8OapYAJ7NBWh_vlvN_-G0boZwefvxSkVZAlxhoFnfcO7yhvjQzM8qSmFKDsaTnULJLHvRzDpIqfgabOt9FmyBOaHz_hcjDMdRy47Ois">Google Scholar</a></span><br>
                  
            <span> <a href=" https://github.com/lliai">GitHub</a></span>, 
                  <span> <a href="https://twitter.com/luking66">Twitter</a></span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me
        </h2>
        <div class="paper">
                I am a researcher of HKGAI and PhD of Hong Kong University of Science and Technology, supervised by Prof. <span> <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo"> Yi-Ke Guo</a></span>. 
                   <br>
                I received my B.Eng. degree in Electrical Engineering from Central South University (CSU). And I was a postgraduate of Institute of Automation, University of Chinese Academy of Science (CASIA). 
                      <br>
                My research focus on Efficient Machine Learning and Large Language Models. I have published some top Machine Learning conferences papers like NeurIPS/ICML/ICLR/CVPR/ICCV etc. 
                   <br>
                   I am/was an Area Chair for BMCV'2024 and IJCNN'2025, a Program Committee member for NeurIPS/ICLR/ICML/CVPR/ICCV/ECCV/ACL/AAAI etc. <br>
            <p style='color:red'><strong>I am looking for internship in summer 2025  and academic/industry position in spring 2026. Please feel free to contact me through the email.</strong></p>
            <!-- <p style='color:red'><strong>I am looking for internship in summer 2025  and academic/industry position in spring 2026. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>

<body>




<body>



<div style="clear: both;">
        <div class="section">
            <h2>Researches</h2>
            <div class="paper">
              
          
            My research interests broadly lie in Efficient Machine Learning and Large Language Model: 
           <br>
            <strong> Efficient MoE:</strong>  MoE-SVD;
           <br>
           <strong> Efficient LLM:</strong> Pruner-Zero (ICML'24), DSA (NeurIPS'24), ALS (NeurIPS'24), STBLLM (ICLR'25), NoRA;
           <br>
           <strong>Automated Efficient ML:</strong> EMQ (ICCV'23), ParZC (AAAI'25) AutoProx (AAAI'23), SasWOT (AAAI'23), Auto-GAS (ECCV'24), AttnZero (ECCV'24);
        <br>
       <strong> Automated Distillation:</strong> DisWOT (CVPR'23), Auto-KD (ICCV'23), KD-Zero (NeurIPS'23), DetKDS (ICML'24), Auto-DAS (ECCV'24);
             <br>
   <strong>Knowledge Distillation:</strong> Tf-FD (ECCV'22), SHAKE (NeurIPS'22), NORM (ICLR'23);
     <p style='color:red'><strong>I am looking for collaborators/RA in efficient LLM. Please feel free to contact me through the email.</strong></p>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2>News
        </h2>
        <div class="paper">
                2025-01: One papers accepted by ICLR2025.  <br>
                2024-12: One papers accepted by AAAI2025.  <br>
                2024-09: Two papers accepted by NeurIPS2024.  <br>
                2024-07: Three papers accepted by ECCV2024.  <br>
                2024-05: Two papers accepted by ICML2024.  <br>
                2024-05: I am serving as an Area Chair for BMVC 2024.  <br>
                2024-04: I obtain the DAAD Postdoc-NeT-AI Fellowship 2024.<br>
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2>Selected Preprints on Large Models
        </h2>
        <div class="paper">
                [<a href="https://arxiv.org/abs/2410.03857">2024-10</a>]: You Know What I'm Saying: Jailbreak Attack via Implicit Reference.  <br>
                [<a href="https://arxiv.org/abs/2408.10280">2024-08</a>]: NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models.  <br>
             [<a href="https://openreview.net/forum?id=Q88y4Vbo-2Z">2024-06</a>]: Sparse KD: Knowledge Distillation for Sparse Models in Constrained Fine-tuning Scenarios.  <br>
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>


<body>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->





<div style="clear: both;">
    <div class="section">
        <br>
        <h3 id="confpapers">First-author Publications (18) : NeurIPS&times4, ICML&times2, ICCV&times2, CVPR&times1, ICLR&times2, ECCV&times4, AAAI&times3</h2>
        <h4 id="confpapers">*: Co-first author,**: Corresponding author or project leadership.</h4>



            <div class="paper"><img class="paper" src="./stbllm.png"
            title="STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs">
        <div><strong>STBLLM:Breaking the 1-Bit Barrier with Structured Binary LLMs</strong><br>
            Peijie Dong, <strong><font color="blue">Lujun Li*</font></strong>, Yuedong Zhong, DaYou Du, Ruibo FAN, Yuhan Chen, Zhenheng Tang, Qiang Wang, Wei Xue, Yike Guo, Xiaowen Chu.<br>
      in  <i>TThe Thirteenth International Conference on Learning Representations </i> <strong>(ICLR-2025)</strong><br>
        <alert>THU-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
        [<a href="https://arxiv.org/abs/2408.01803">Paper</a>]
        [<a href="https://arxiv.org/abs/2408.01803">Code</a>]
                <br>
               We introduces STBLLM, a novel approach that breaks the 1-bit barrier in language models by leveraging Structured Binary LLMs.
        </div>
        <div class="spanner"></div>
        </div>


        
                <div class="paper"><img class="paper" src="./parzc.png"
            title=" ParZC: Parametric Zero-Cost Proxies for Efficient NAS">
        <div><strong>ParZC: Parametric Zero-Cost Proxies for Efficient NAS</strong><br>
            Peijie Dong, <strong><font color="blue">Lujun Li*</font></strong>, Zhenheng Tang, Zimian Wei, Xiang Liu,  Qiang Wang, Xiaowen Chu.<br>
      in  <i>Thirty-Ninth AAAI Conference on Artificial Intelligence </i> <strong>(AAAI-2025)</strong><br>
        <alert>CCF-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
        [<a href="https://arxiv.org/abs/2402.02105">Paper</a>]
        [<a href="https://arxiv.org/abs/2402.02105">Code</a>]
                <br>
               Our Parametric Zero-Cost Proxies (ParZC) improves zero-shot Neural Architecture Search by addressing unequal node importance and using novel techniques for uncertainty estimation and architecture ranking..
        </div>
        <div class="spanner"></div>
        </div>
        
        <div class="paper"><img class="paper" src="./DSA.png"
            title="Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models">
        <div><strong>Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Peijie Dong, Zhenheng Tang, Xiang Liu, Qiang Wang, Wenhan Luo, Wei Xue, Qifeng Liu, Xiaowen Chu, Yike Guo.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2024)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/forum?id=rgtrYVC9n4">Paper</a>]
        [<a href=" https://github.com/lliai/DSA">Code</a>]
                <br>
                In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models.
        </div>
        <div class="spanner"></div>
        </div>
        <div class="paper"><img class="paper" src="./ALS.jpg"
            title="Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment">
        <div><strong></strong>Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment</strong><br>
        Wei Li, <strong><font color="blue">Lujun Li** </font></strong>, Mark G. Lee, Shengjie Sun.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2024)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/forum?id=Jup0qZxH7U">Paper</a>]
        [<a href="https://github.com/lliai/ALS">Code</a>]
                <br>
                In this paper, we present present an approach called Adaptive Layer Sparsity  for optimizing large language models by selectively pruning features in intermediate layers. .
        </div>
        <div class="spanner"></div>
        </div>
                <div class="paper"><img class="paper" src="./PrunerZero.png"
            title=" Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models">
        <div><strong>Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models</strong><br>
            Peijie Dong, <strong><font color="blue">Lujun Li*</font></strong>, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu.<br>
            in </i> International Conference on Machine Learning </i> <strong>(ICML-2024)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/pdf?id=1tRLxQzdep">Paper</a>]
        [<a href="https://github.com/pprp/Pruner-Zero">Code</a>]
                <br>
                We propose Pruner-Zero framework to automatically devise pruning metrics for post-training pruning LLMs.
        </div>
        <div class="spanner"></div>
        </div>
        <div class="paper"><img class="paper" src="./DetKDS.png"
            title=" DetKDS: Knowledge Distillation Search for Object Detectors">
        <div><strong>DetKDS: Knowledge Distillation Search for Object Detectors</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Yufan Bao, Peijie Dong, Chuanguang Yang, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo.<br>
      in </i> International Conference on Machine Learning </i> <strong>(ICML-2024)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/pdf/fcca99569d2c5ab0653814d94a6625b2e27ef2a2.pdf">Paper</a>]
        [<a href="https://github.com/lliai/DetKDS">Code</a>]
                <br>
                In this paper, we present DetKDS, the first knowledge distillation search framework to enhance any detectors by searching for optimal distillation policies.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./AttnZero.png"
            title="AttnZero: Efficient Attention Discovery for Vision Transformers">
        <div><strong>AttnZero: Efficient Attention Discovery for Vision Transformers</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Zimian Wei, Peijie Dong, Wenhan Luo, Wei Xue,Qifeng Liu,Yike Guo.<br>
      in </i> European Conference on Computer Vision 2024 </i> <strong>(ECCV-2024)</strong><br>
        <alert>Top Conference in Computer Vision</alert>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00666.pdf">Paper</a>]
        [<a href="https://github.com/lliai/AttnZero">Code</a>]
                <br>
                In this paper, we present AttnZero, the first framework for automatically discovering efficient attention modules tailored for Vision Transformers (ViTs).
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./AutoGAS.png"
            title=" Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search">
        <div><strong>Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Haosen Sun, Shiwen Li, Peijie Dong, Wenhan Luo, Wei Xue,Qifeng Liu,Yike Guo.<br>
      in </i> European Conference on Computer Vision 2024 </i> <strong>(ECCV-2024)</strong><br>
        <alert>Top Conference in Computer Vision</alert>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00668.pdf">Paper</a>]
        [<a href="https://github.com/lliai/Auto-GAS">Code</a>]
                <br>
               In this paper, we introduce Auto-GAS, the first training-free Generative Architecture Search (GAS) framework enabled by an auto-discovered proxy .
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./AutoDAS.png"
            title="Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search">
        <div><strong>Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search</strong><br>
        Haosen Sun, <strong><font color="blue">Lujun Li </font></strong>,  Peijie Dong,Zimian Wei, Shitong Shao.<br>
      in </i> European Conference on Computer Vision 2024 </i> <strong>(ECCV-2024)</strong><br>
        <alert>Top Conference in Computer Vision</alert>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00676.pdf">Paper</a>]
        [<a href="https://github.com/lliai/Auto-DAS">Code</a>]
                <br>
                In this paper, we present Auto-DAS, an automatic proxy discovery framework using an Evolutionary Algorithm (EA) for training-free DAS.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./kd-zero.png"
            title=" KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs">
        <div><strong>KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Peijie Dong, Anggeng Li, Zimian Wei, Ya Yang.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2023)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/pdf?id=OlMKa5YZ8e">Paper</a>]
        [<a href="https://openreview.net/pdf?id=OlMKa5YZ8e">Code</a>]
                <br>
        We present KD-Zero, the first auto-search framework for evolving best distiller from scratch to alleviate teacher-student gaps..
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./autokd.png"
            title=" Automated Knowledge Distillation via Monte Carlo Tree Search">
        <div><strong> Automated Knowledge Distillation via Monte Carlo Tree Search</strong><br>
       <strong><font color="blue">Lujun Li </font></strong>, Peijie Dong, Zimian Wei, Ya Yang.<br>
        in  <i>The International Conference on Computer Vision </i> <strong>(ICCV-2023)</strong>,<br>
        <alert>CCF-A,   Top Conference in  Computer Vision</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/liai/Auto-KD">Code</a>]
                <br>
        In this paper, we present Auto-KD, the first automated search framework for optimal knowledge distillation design.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./EMQ.png"
            title=" EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization.">
        <div><strong> EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization</strong><br>
            Peijie Dong, <strong><font color="blue">Lujun Li*</font></strong>, Zimian Wei, Xin Niu, Zhiliang Tian, Hengyue Pan.<br>
      in  <i>The International Conference on Computer Vision </i> <strong>(ICCV-2023)</strong><br>
        <alert>CCF-A,   Top Conference in  Computer Vision</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/liai/EMQ-series">Code</a>]
                <br>
         We first build the MQ-Bench-101 and develop an automatic search of proxies framework for MQ via evolving algorithms.
        
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./autoprox.png"
            title=" Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery.">
        <div><strong> Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery</strong><br>
            Zimian Wei,  <strong><font color="blue">Lujun Li*</font></strong>, Peijie Dong, Zheng Hui,  Anggeng Li,  Menglong Lu, Hengyue Pan, Dongsheng Li.<br>
      in  <i>Thirty-Eighth AAAI Conference on Artificial Intelligence </i> <strong>(AAAI-2024)</strong><br>
        <alert>CCF-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
                [<a href="https://arxiv.org/abs/2402.02105">Paper</a>]
                [<a href="https://github.com/lliai/Auto-Prox-AAAI24">Code</a>]
                <br>
         We first build the ViT-Bench-101 and develop zero-cost proxy search for Vision Transformer  on multiple datasets.
        
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./saswot.png"
            title=" SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training.">
        <div><strong> SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training</strong><br>
            Chendi Zhu,  <strong><font color="blue">Lujun Li*</font></strong>, Yuli Wu, Zheng Hui,  Zhengxing Sun.<br>
      in  <i>Thirty-Eighth AAAI Conference on Artificial Intelligence </i> <strong>(AAAI-2024)</strong><br>
        <alert>CCF-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
              [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28606/29177">Paper</a>]
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28606/29177">Code</a>]
                <br>
        We present the first training-free architecture search framework for Real-time Semantic Segmentation.
        
        </div>
        <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="./shake.png"
                                title=" Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer">
            <div><strong> Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer</strong><br>
               <strong><font color="blue">Lujun Li </font></strong>, Zhe Jin.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2022)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
                <br>
                [<a href="https://openreview.net/pdf?id=prQT0gN81oG">Paper</a>]
                [<a href="https://github.com/liai/SHAKE">Code</a>]
                <br>
               We present SHAKE with reversed distillation and shadow head to bridge offline and online knowledge transfer, achieving superior performance in multiple tasks and scenarios.
             
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./tffd.png"
                                title="Self-Regulated Feature Learning via Teacher-free Feature Distillation">
            <div><strong>Self-Regulated Feature Learning via Teacher-free Feature Distillation</strong><br>
                <strong><font color="blue">Lujun Li </font></strong>.<br>
                in <i>European Conference on Computer Vision </i> <strong> (ECCV-2022) <strong><br>
                <alert>CCF-B;  Top Conference in  Computer Vision</alert>
              <br>
                [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860337.pdf">Paper</a>]
                [<a href="https://github.com/liai/Teacher-free-Distillation">Code</a>]
                <br>
                We we propose Tf-FD for reusing channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model.
           
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./diswot.png"
                                title=" DisWOT: Student Architecture Search for Distillation WithOut Training">
            <div><strong> DisWOT: Student Architecture Search for Distillation WithOut Training</strong><br>
                 Peijie Dong, <strong><font color="blue">Lujun Li***</font></strong>, Zimian Wei<br>
                in                     <i>IEEE / CVF Computer Vision and Pattern Recognition Conference </i> <strong>(CVPR-2023)</strong><br>
           <alert>CCF-A,   Top Conference in  Computer Vision</alert>
                <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/liai/DisWOT-CVPR2023">Code</a>]
                <br>
                <br>
                 We propose a strong self-supervised augmented knowledge distillation method from hierarchical feature maps for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./norm.png"
                                title="   NORM: Knowledge Distillation via N-to-One Representation Matching">
            <div><strong>   NORM: Knowledge Distillation via N-to-One Representation Matching</strong><br>
                 Xiaolong Liu, <strong><font color="blue">Lujun Li*</font></strong>, Chao Li, Anbang Yao<br>
                in   <i>International Conference on Learning Representations </i> <strong>(ICLR-2023)</strong><br>
                <alert>Top Conference in Machine Learning</alert><br>
                [<a href="https://openreview.net/pdf?id=CRNwGauQpb6">Paper</a>]
                [<a href="https://openreview.net/attachment?id=CRNwGauQpb6&name=supplementary_material">Code</a>]
                <br>
                <br>
                 We presents a new knowledge distillation method via n-to-one representation matching.
            </div>
            <div class="spanner"></div>
        </div>


</body>
<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
                <li>
                DAAD Postdoc-NeT-AI Fellowship 2024
                    <li>
                    Top 3 in CVPR2022 Second lightweight NAS challenge supernet Track, 2022
                </li>
                <li>
                    Outstanding entrepreneurial undergraduate, 2019
                </li>
                <li>
                    Second Prize (National-level)  China Undergraduate electronic design competition, 2017,2018
                </li>
                <li>
                    MCM/ICM -- Honorable Mention, 2016
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2>Services</h2>
        <div class="paper">
          
            Area Chair: 
   <li>
                 International Joint Conference on Neural Networks (IJCNN), 2025
            <li>
                
            British Machine Vision Conference (BMVC), 2024

           
            <br>
            Conference Review: 
                <li>
            2025: CVPR, ICML, IJCAI, AutoML, ACL;
                        <li>
            2024: ACM MM, ECCV, ICML,  NeurIPS, ICLR, AAAI, AutoML, ACL;
    <li>

            2023: CVPR, AAAI,  ACM MM, ICCV, NeurIPS, ICLR, WACV, AutoML;
            <li>
    
           2022: CVPR, AAAI,  ACM MM, ECCV, NeurIPS, WACV;

              <li>
    
            2021: AAAI, ACM MM; 
            <br>

            Journal Review:
            <li>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
            <li>
International Journal of Computer Vision (IJCV)
<li>
IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
<li>
IEEE Transactions on Circuits System and Video Technology (TCSVT)


        </div>
    </div>
</div>


</body>
</html>
