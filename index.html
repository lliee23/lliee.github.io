<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Luking Li</title>
    <meta content="Luking Li, https://github/lilujunai" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Times New Roman;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Times New Roman;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Times New Roman;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Times New Roman;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Times New Roman;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 110px;"
             src="./lujunli.jpg">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Lujun LI (李路军)</span><br>
            <span> Hong Kong University of Science and Technology (HKUST)</span><br>
<span><strong>Address</strong>: Clear Water Bay Peninsula, New Territories, Hong Kong</span><br>
            <span><strong>Email</strong>: lilujunai@gmail.com, lliee@connect.ust.hk </span> <br>
            <span> <a href="https://twitter.com/luking66">Twitter</a></span> <br>
            <span> <a href="https://github.com/lilujunai">GitHub</a></span> <br>
            <span> <a href="https://scholar.google.com/citations?hl=en&user=aPl3DjIAAAAJ&view_op=list_works&gmla=AJsN-F6PzWqNjlzy8OapYAJ7NBWh_vlvN_-G0boZwefvxSkVZAlxhoFnfcO7yhvjQzM8qSmFKDsaTnULJLHvRzDpIqfgabOt9FmyBOaHz_hcjDMdRy47Ois">Google Scholar</a></span>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me
        </h2>
        <div class="paper">
                I am a PhD student of the University of Science and Technology supervised by Prof. <span> <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo">Yi-Ke Guo</a></span>. 
                   I am also a researcher of HKGAI.   <br>
                I received my B.Eng. degree in Electrical Engineering from Central South University (CSU). And I was a postgraduate of Institute of Automation, University of Chinese Academy of Science (CASIA). 
                      <br>
                My research focus on Efficient Generative AI. I have published some top conferences papers in CVPR/ICCV2/NeurIPS/ICLR/ECCV/AAAI etc. 
                   <br>
                In addition, I also serve as reviewers for NeurIPS/ICLR/ICML/CVPR/ICCV/ECCV/ACL/AAAI etc. <br>
      <br>
      <strong>News: </strong><strong><font color="red">Three papers accepted by AAAI24!</font></strong>
      <br>
            My research interests broadly lie in: 
            <li>

            General Efficient AI (e.g., Knowledge Distillation, Quantization, Pruning, etc), 
            <li>
    
            Automated Machine Learning (NAS, KDS, VIT/Attention Search, Symbol Search, etc), 
              <li>
    
            Generative AI (Generate Models, Images & Video Generation, etc).
          <li>
    
            Large Language Model (Knowledge Distillation, Pruning, MoE, LoRA etc ).
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>

<body>


<div style="clear: both;">
    <div class="section">
        <h2>News！
        </h2>
        <div class="paper">
                
                12/23: Three papers accepted by AAAI24! 
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>

<body>





<body>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->





<div style="clear: both;">
    <div class="section">
        <br>
        <h2 id="confpapers">First-authored Publications (9) : NeurIPS&times2, ICCV&times2, CVPR&times1, ICLR&times1, ECCV&times1, AAAI&times2</h2>
        <h4 id="confpapers">*: Co-first author,**: Corresponding author.</h4>

        <div class="paper"><img class="paper" src="./kd-zero.png"
            title=" KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs">
        <div><strong>KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs</strong><br>
        <strong><font color="blue">Lujun Li </font></strong>, Peijie Dong, Anggeng Li, Zimian Wei, Ya Yang.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2023)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
        <br>
        [<a href="https://openreview.net/pdf?id=OlMKa5YZ8e">Paper</a>]
        [<a href="https://github.com/lilujunai/KD-Zero">Code</a>]
                <br>
        We present KD-Zero, the first auto-search framework for evolving best distiller from scratch to alleviate teacher-student gaps..
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./autokd.png"
            title=" Automated Knowledge Distillation via Monte Carlo Tree Search">
        <div><strong> Automated Knowledge Distillation via Monte Carlo Tree Search</strong><br>
       <strong><font color="blue">Lujun Li </font></strong>, Peijie Dong, Zimian Wei, Ya Yang.<br>
        in  <i>The International Conference on Computer Vision </i> <strong>(ICCV-2023)</strong>,<br>
        <alert>CCF-A,   Top Conference in  Computer Vision</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/lilujunai/Auto-KD">Code</a>]
                <br>
        In this paper, we present Auto-KD, the first automated search framework for optimal knowledge distillation design.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./EMQ.png"
            title=" EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization.">
        <div><strong> EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization</strong><br>
            Peijie Dong, <strong><font color="blue">Lujun Li*</font></strong>, Zimian Wei, Xin Niu, Zhiliang Tian, Hengyue Pan.<br>
      in  <i>The International Conference on Computer Vision </i> <strong>(ICCV-2023)</strong><br>
        <alert>CCF-A,   Top Conference in  Computer Vision</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/lilujunai/EMQ-series">Code</a>]
                <br>
         We first build the MQ-Bench-101 and develop an automatic search of proxies framework for MQ via evolving algorithms.
        
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./autoprox.png"
            title=" Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery.">
        <div><strong> Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery</strong><br>
            Zimian Wei,  <strong><font color="blue">Lujun Li*</font></strong>, Peijie Dong, Zheng Hui,  Anggeng Li,  Menglong Lu, Hengyue Pan, Dongsheng Li.<br>
      in  <i>Thirty-Eighth AAAI Conference on Artificial Intelligence </i> <strong>(AAAI-2024)</strong><br>
        <alert>CCF-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
                [<a >Paper coming</a>]
                [<a >Code coming</a>]
                <br>
         We first build the ViT-Bench-101 and develop zero-cost proxy search for Vision Transformer  on multiple datasets.
        
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./saswot.png"
            title=" SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training.">
        <div><strong> SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training</strong><br>
            Chendi Zhu,  <strong><font color="blue">Lujun Li*</font></strong>, Yuli Wu, Zheng Hui,  Zhengxing Sun.<br>
      in  <i>Thirty-Eighth AAAI Conference on Artificial Intelligence </i> <strong>(AAAI-2024)</strong><br>
        <alert>CCF-A,   Top Conference in  Artificial Intelligence</alert>
        <br>
                [<a >Paper coming</a>]
                [<a >Code coming</a>]
                <br>
        We present the first training-free architecture search framework for Real-time Semantic Segmentation.
        
        </div>
        <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="./shake.png"
                                title=" Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer">
            <div><strong> Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer</strong><br>
               <strong><font color="blue">Lujun Li </font></strong>, Zhe Jin.<br>
      in </i> Conference on Neural Information Processing Systems </i> <strong>(NeurIPS-2022)</strong><br>
        <alert>CCF-A, Top Conference in Machine Learning</alert>
                <br>
                [<a href="https://openreview.net/pdf?id=prQT0gN81oG">Paper</a>]
                [<a href="https://github.com/lilujunai/SHAKE">Code</a>]
                <br>
               We present SHAKE with reversed distillation and shadow head to bridge offline and online knowledge transfer, achieving superior performance in multiple tasks and scenarios.
             
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./tffd.png"
                                title="Self-Regulated Feature Learning via Teacher-free Feature Distillation">
            <div><strong>Self-Regulated Feature Learning via Teacher-free Feature Distillation</strong><br>
                <strong><font color="blue">Lujun Li </font></strong>.<br>
                in <i>European Conference on Computer Vision </i> <strong> (ECCV-2022) <strong><br>
                <alert>CCF-B;  Top Conference in  Computer Vision</alert>
              <br>
                [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860337.pdf">Paper</a>]
                [<a href="https://github.com/lilujunai/Teacher-free-Distillation">Code</a>]
                <br>
                We we propose Tf-FD for reusing channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model.
           
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./diswot.png"
                                title=" DisWOT: Student Architecture Search for Distillation WithOut Training">
            <div><strong> DisWOT: Student Architecture Search for Distillation WithOut Training</strong><br>
                 Peijie Dong, <strong><font color="blue">Lujun Li***</font></strong>, Zimian Wei<br>
                in                     <i>IEEE / CVF Computer Vision and Pattern Recognition Conference </i> <strong>(CVPR-2023)</strong><br>
           <alert>CCF-A,   Top Conference in  Computer Vision</alert>
                <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf">Paper</a>]
                [<a href="https://github.com/lilujunai/DisWOT-CVPR2023">Code</a>]
                <br>
                <br>
                 We propose a strong self-supervised augmented knowledge distillation method from hierarchical feature maps for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./norm.png"
                                title="   NORM: Knowledge Distillation via N-to-One Representation Matching">
            <div><strong>   NORM: Knowledge Distillation via N-to-One Representation Matching</strong><br>
                 Xiaolong Liu, <strong><font color="blue">Lujun Li*</font></strong>, Chao Li, Anbang Yao<br>
                in   <i>International Conference on Learning Representations </i> <strong>(ICLR-2023)</strong><br>
                <alert>Top Conference in Machine Learning</alert><br>
                [<a href="https://openreview.net/pdf?id=CRNwGauQpb6">Paper</a>]
                [<a href="https://openreview.net/attachment?id=CRNwGauQpb6&name=supplementary_material">Code</a>]
                <br>
                <br>
                 We presents a new knowledge distillation method via n-to-one representation matching.
            </div>
            <div class="spanner"></div>
        </div>


</body>
<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
                    <li>
                    Top 3 in CVPR2022 Second lightweight NAS challenge supernet Track, 2022
                </li>
                <li>
                    Outstanding entrepreneurial undergraduate, 2019
                </li>
                <li>
                    Second Prize (National-level)  China Undergraduate electronic design competition, 2017,2018
                </li>
                <li>
                    MCM/ICM -- Honorable Mention, 2016
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            Journal: IJCV;TNNLS;TCSVT
            Conference: 
                <li>

            2024: ACM MM, ECCV, ICML, AutoML, ACL ;
    <li>

            2023: CVPR, AAAI,  ACM MM, ICCV, NeurIPS, ICLR, WACV, AutoML;
            <li>
    
           2022: CVPR, AAAI,  ACM MM, ECCV, NeurIPS, WACV;

              <li>
    
            2021: AAAI, ACM MM; 
                      <li>

        </div>
    </div>
</div>


</body>
</html>
